{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fda3b87-3796-4b24-b902-2f0d500e53d0",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8851c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d4c9f",
   "metadata": {},
   "source": [
    "# Load Dataset and Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47faab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and display the first five rows\n",
    "df = pd.read_csv(\"student_data_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb672e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics of the training dataset\n",
    "print(\"Summary Statistics of Training Data\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91465a-ac7c-4afb-9644-a7ff4268d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training dataset\n",
    "print(\"Checking for missing valuse in Training Data\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27c15f",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70214945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to numerical using Label Encoding\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f1b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and target variable\n",
    "X = df.drop('Performance', axis=1)\n",
    "y = df['Performance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b80ba-c3ee-4066-a004-bf5ef4b2602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72b8b1",
   "metadata": {},
   "source": [
    "# Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the decision tree classifier\n",
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6954ce04",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0422b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on the test data\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bcafb4",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print metrics \n",
    "print(\"Accuracy::\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision::\", metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall::\", metrics.recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1-Score::\", metrics.f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View classification report\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5630ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "joblib.dump(clf, 'student_performance_predictor.joblib')\n",
    "\n",
    "# Save the label encoders\n",
    "joblib.dump(label_encoders, 'label_encoders.joblib')\n",
    "\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47db3ba",
   "metadata": {},
   "source": [
    "# Calculate Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f8edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "# Create a DataFrame to display feature importances\n",
    "feature_importances = pd.DataFrame({'Feature' : X.columns, 'Importance' : importances})\n",
    "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c594c790",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c00807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Decision Tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(clf, filled=True, feature_names=X.columns, class_names=label_encoders['Performance'].classes_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77119197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6)) \n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues') \n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.jpg', dpi=300)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(feature_importances['Feature'], feature_importances['Importance'], marker='o')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.grid(True)\n",
    "plt.savefig('feature_importance.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for numerical features\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64'])\n",
    "numerical_features.hist(figsize=(12, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_visualizations.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.savefig('correlation_heatmap.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd688998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correlations with Performance\n",
    "correlations = df.corr()['Performance'].drop('Performance')\n",
    "correlations = correlations.sort_values(ascending=False)\n",
    "\n",
    "# Plot the correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlations.plot(kind='bar')\n",
    "plt.title('Correlation with Performance')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.savefig('feature_correlation.jpg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44a1b5",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199db366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "df_test = pd.read_csv(\"student_data_test.csv\")\n",
    "\n",
    "# Apply label encoding to categorical variables\n",
    "for column in df_test.select_dtypes(include=['object']).columns:\n",
    "    df_test[column] = label_encoders[column].transform(df_test[column])\n",
    "\n",
    "# Split the test dataset into features and target variable\n",
    "X_test = df_test.drop('Performance', axis=1)\n",
    "y_test = df_test['Performance']\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy on Test Data:\", metrics.accuracy_score(y_test, y_pred_test))\n",
    "print(\"Precision on Test Data:\", metrics.precision_score(y_test, y_pred_test, average='weighted'))\n",
    "print(\"Recall on Test Data:\", metrics.recall_score(y_test, y_pred_test, average='weighted'))\n",
    "print(\"F1-Score on Test Data:\", metrics.f1_score(y_test, y_pred_test, average='weighted'))\n",
    "print(metrics.classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_test), annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix on Test Data')\n",
    "plt.savefig('confusion_matrix_test.jpg', dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
